{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1789568-5b10-4bcd-9da9-3667bdd348dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook offers an example of building a multi-label text classifier on student survey responses\n",
    "#Note the sample data has unique labels for each response, making this a multiclass classification problem\n",
    "#However, in practice, most survey responses will be able to take more than one label, making it a multilabel classification problem\n",
    "#Since the actual student data is protected by FERPA and cannot be shared, we are only using fake responses generated by ChatGPT\n",
    "#we do not have enough data to train a successful model or evaluate the model, but this provides the structure of how our model was trained.\n",
    "\n",
    "#import packages\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#This is a dataset of fake student survey responses generated with ChatGPT\n",
    "surveys = pd.read_excel(\"Topics/fake_student_survey_data.xlsx\")\n",
    "surveys_all = Dataset.from_pandas(surveys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adba7ec2-346d-46a4-b073-9fa903b8fc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '    \"Create innovation hubs that support entrepreneurial ventures. Pursuing my business ideas was challenging without dedicated resources and mentorship.\"',\n",
       " 'Academic': 0,\n",
       " 'Facilities': 0,\n",
       " 'Career': 1,\n",
       " 'Financial': 0,\n",
       " 'Diversity': 0,\n",
       " 'Wellness': 0,\n",
       " 'Social': 0,\n",
       " 'Technology': 0,\n",
       " 'Sustainability': 0,\n",
       " 'Policy': 0,\n",
       " 'Communication': 0,\n",
       " 'Ethics': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = surveys_all[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2da1be-dddc-4619-8689-58533f764161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Academic',\n",
       " 'Facilities',\n",
       " 'Career',\n",
       " 'Financial',\n",
       " 'Diversity',\n",
       " 'Wellness',\n",
       " 'Social',\n",
       " 'Technology',\n",
       " 'Sustainability',\n",
       " 'Policy',\n",
       " 'Communication',\n",
       " 'Ethics']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in surveys_all.features.keys() if label not in ['text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b2f7d76-cd22-49f2-be16-813da60f0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"roberta-base\"\n",
    "#model_path = \"microsoft/deberta-v3-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ff5775-a21b-4c0d-8aee-dd96ca018c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5f66c15-9ec3-43ca-99dd-1ac72aa3d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "  # get the survey responses\n",
    "  text = examples[\"text\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752c4ac9-2e3d-4a65-baee-4fe85c920038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12030ce0363b485a95364b7bb9f01d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = surveys_all.map(preprocess_data, batched=True, remove_columns=surveys_all.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3534bbef-b9da-499a-b401-0132fb39606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset[0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a8104ef-cc82-4640-9617-41588db9c30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>    \"Create innovation hubs that support entrepreneurial ventures. Pursuing my business ideas was challenging without dedicated resources and mentorship.\"</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4da11060-fd40-48ed-a526-7adeb1c19c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acfa46d6-487b-49c6-b7dd-2683636c0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8051b6e-ceb2-42a5-b35a-55ba78e063c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "303b56e2-1a02-41a2-b0a5-6c225a9c4a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "multi_label_model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5244b6c4-28e5-41b9-b8bc-4cee529b1b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 09:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.358615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.300925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.286754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.282374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.280807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=115, training_loss=0.35201333087423575, metrics={'train_runtime': 560.9885, 'train_samples_per_second': 0.802, 'train_steps_per_second': 0.205, 'total_flos': 29602651392000.0, 'train_loss': 0.35201333087423575, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set hyperparameters and train the model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "multi_label_training_args = TrainingArguments(\n",
    "    model_path,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "multi_label_trainer = Trainer(\n",
    "    multi_label_model,\n",
    "    multi_label_training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset, #Here we have not evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "multi_label_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ce282-73ac-4182-8480-e18fce807243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
